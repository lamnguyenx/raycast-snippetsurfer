# HeavyDutySnippet Architecture Plan

**Date**: 2025-11-28
**Version**: 01
**Status**: Ready for Implementation

## Problem Statement

The current SnippetSurfer extension loads entire markdown/yaml files into memory, causing performance issues and memory exhaustion with large files (>10MB). The extension cannot handle files of significant size due to:

1. **Full Content Loading**: Reads entire files upfront during directory scanning
2. **Memory Storage**: Stores all content in memory for display and copy operations
3. **Complex Parsing**: Processes markdown frontmatter and YAML structures for every file
4. **Synchronous Operations**: Blocks UI while loading large directories

## Solution Overview: HeavyDutySnippet Architecture

Transform the extension into a **lazy-loading, memory-efficient file browser** that handles files of any practical size while maintaining responsive UX.

### Core Principles

1. **Zero Preloading**: Nothing loaded into memory until explicitly needed
2. **Pure Text Display**: No markdown/YAML parsing - just raw text
3. **On-Demand Operations**: Load content only when user requests it
4. **Subprocess Integration**: Use OS clipboard directly for copy operations

## Technical Architecture

### Data Structures

```typescript
interface HeavyDutySnippet {
  id: string; // MD5 hash of fullPath
  name: string; // filename without extension
  folder: string; // relative path from scan root
  fullPath: string; // absolute path for lazy loading
  fileSize: number; // bytes, for UI indicators and limits
  modifiedTime: Date; // for sorting and change detection
}
```

### Three-Phase Loading Strategy

#### Phase 1: Discovery (Zero Memory)

- **Input**: Directory paths from preferences
- **Process**: `fs.readdir()` + `fs.stat()` only
- **Output**: `HeavyDutySnippet[]` with metadata only
- **Memory**: O(number of files × metadata size)

```typescript
async function discoverSnippets(paths: string[]): Promise<HeavyDutySnippet[]> {
  const snippets: HeavyDutySnippet[] = [];

  for (const rootPath of paths) {
    await scanDirectory(rootPath, snippets);
  }

  return snippets.sort((a, b) => b.modifiedTime.getTime() - a.modifiedTime.getTime());
}

async function scanDirectory(dirPath: string, snippets: HeavyDutySnippet[]): Promise<void> {
  const entries = await fs.promises.readdir(dirPath, { withFileTypes: true });

  for (const entry of entries) {
    if (entry.name.startsWith(".")) continue;

    const fullPath = path.join(dirPath, entry.name);

    if (entry.isDirectory()) {
      await scanDirectory(fullPath, snippets);
    } else if (isSupportedFile(entry.name)) {
      const stats = await fs.promises.stat(fullPath);
      const snippet = createHeavyDutySnippet(fullPath, dirPath, stats);
      snippets.push(snippet);
    }
  }
}
```

#### Phase 2: Preview Loading (Minimal Memory)

- **Trigger**: User selects snippet in Raycast list
- **Process**: Stream first 50 lines of text
- **Cache**: LRU cache with 50MB limit
- **Memory**: O(number of viewed snippets × 2KB)

```typescript
async function loadSnippetPreview(snippet: HeavyDutySnippet): Promise<string> {
  // Check cache first
  const cached = previewCache.get(snippet.id);
  if (cached) return cached;

  // Stream first ~2KB (approximately 50 lines)
  const stream = fs.createReadStream(snippet.fullPath, {
    encoding: "utf8",
    start: 0,
    end: 2048,
  });

  let content = "";
  for await (const chunk of stream) {
    content += chunk;
  }

  const lines = content.split("\n").slice(0, 50);
  const preview = lines.join("\n");

  // Truncate indicator for large files
  if (snippet.fileSize > 1024 * 1024) {
    // 1MB+
    preview += "\n\n[... Large file - use copy action for full content ...]";
  }

  // Cache the result
  previewCache.set(snippet.id, preview);
  return preview;
}
```

#### Phase 3: Action Execution (Zero Memory)

- **Copy/Paste**: Direct subprocess to OS clipboard
- **Open File**: OS file opening
- **Memory**: O(1) - no content stored in Node.js

```typescript
async function copySnippetToClipboard(snippet: HeavyDutySnippet): Promise<void> {
  const { spawn } = require("child_process");

  return new Promise((resolve, reject) => {
    // Platform-specific clipboard commands
    const command = process.platform === "darwin" ? "pbcopy" : "xclip -selection clipboard";

    const child = spawn("sh", ["-c", `cat "${snippet.fullPath}" | ${command}`]);

    child.on("close", (code: number) => {
      if (code === 0) {
        resolve();
      } else {
        reject(new Error(`Clipboard command failed with code ${code}`));
      }
    });

    child.on("error", reject);
  });
}
```

## Implementation Plan

### Phase 1: Core Infrastructure

1. Create `HeavyDutySnippet` interface
2. Implement discovery-only loading
3. Update main search component to use new structure
4. Basic UI with filename/folder display

### Phase 2: Lazy Preview System

1. Add preview loading on selection
2. Implement LRU cache for previews
3. Update detail view to show text previews
4. Add loading states and error handling

### Phase 3: Action System

1. Implement subprocess copy/paste
2. Add file opening actions
3. Platform-specific command handling
4. Error handling for subprocess failures

### Phase 4: Optimization & Polish

1. Add file size indicators and warnings
2. Implement search optimizations
3. Add configuration options
4. Performance testing with large file sets

## Performance Targets

- **Startup Time**: <100ms for 10,000 files
- **Memory Usage**: <50MB for normal usage patterns
- **Large File Handling**: No performance degradation for 100GB+ files
- **UI Responsiveness**: Instant list navigation, <500ms preview loading

## Compatibility & Migration

### Backward Compatibility

- Existing preferences and configuration preserved
- Graceful fallback for edge cases
- No breaking changes to user workflows

### File Format Support

- **Supported**: Any text file (.md, .txt, .yaml, .yml, custom extensions)
- **Filtered Out**: Binary files, hidden files, unsupported formats
- **Size Limits**: Configurable via preferences (default: no limit)

### Error Handling

- File access permissions
- Encoding issues (UTF-8 fallback)
- Subprocess failures
- Cache corruption recovery

## Benefits

1. **Infinite Scale**: Handles any practical file size
2. **Instant Startup**: No content preloading delays
3. **Memory Efficient**: Proportional to actual usage
4. **Simple Architecture**: No complex parsing logic
5. **Platform Native**: Leverages OS clipboard performance
6. **Future Proof**: Easy to extend for new file types

## Risk Mitigation

### Technical Risks

- **Subprocess Security**: Proper escaping of file paths
- **Platform Compatibility**: Comprehensive OS detection
- **Memory Leaks**: Proper cache management and cleanup
- **Race Conditions**: Atomic operations for cache updates

### User Experience Risks

- **Learning Curve**: Clear UI indicators for lazy loading
- **Performance Perception**: Loading indicators and progress feedback
- **Feature Parity**: Ensure all current functionality preserved

## Success Metrics

- **Performance**: Startup time <100ms, memory <50MB
- **Reliability**: Zero crashes with large files
- **Usability**: Intuitive lazy loading UX
- **Compatibility**: Works on macOS, Linux, Windows

## Conclusion

The HeavyDutySnippet architecture transforms SnippetSurfer from a memory-bound content parser into a scalable, efficient file browser. By embracing lazy loading and subprocess integration, it can handle files of any size while maintaining the responsive, intuitive experience users expect.

This solution not only fixes the immediate large file problem but establishes a foundation for future enhancements and ensures the extension remains performant as snippet collections grow.
